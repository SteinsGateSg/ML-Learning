{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6e1d10",
   "metadata": {},
   "source": [
    "## 一、`nn.Module` 是什么？\n",
    "\n",
    "`nn.Module` 是一个“带参数的可调用对象”的基类，它可以：\n",
    "* 管理模型里的**参数**（权重、偏置）\n",
    "* 管理模型里的**子模块**（层、子网络）\n",
    "* 统一提供一些方法（`parameters()`、`to()`、`train()`、`eval()` 等）\n",
    "\n",
    "我们的网络基本都长这样：\n",
    "\n",
    "```python\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 在这里定义“结构”（子模块 / 层）\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 在这里定义“数据是如何流过这些结构的”\n",
    "        return x\n",
    "```\n",
    "\n",
    "* `__init__` 里定义**需要学习的层 / 子模块**（`nn.Linear`, `nn.Conv2d` 等）\n",
    "* `forward` 里定义**前向计算流程**\n",
    "* 不需要手写 `backward`，PyTorch 自动利用 autograd 通过 `forward` 的计算图求梯度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d576a",
   "metadata": {},
   "source": [
    "## 二. 一个使用模板\n",
    "\n",
    "```py\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 初始化基类\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)               # [batch, 1, 28, 28] -> [batch, 784]\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits                     # [batch, 10]\n",
    "\n",
    "model = MLP()\n",
    "```\n",
    "注意：\n",
    "1. `class MLP(nn.Module)`：继承基类\n",
    "2. `super().__init__()`：调用父类构造函数（不写会出各种怪问题）\n",
    "3. 在 __init__ 里把层“挂”在 `self.xxx` 上：\n",
    "    - 只要是赋给 `self. `的 `nn.Module` 或 `nn.Parameter`，PyTorch 会自动登记为参数 / 子模块\n",
    "4. 在 `forward` 里使用这些层完成前向计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e927d54",
   "metadata": {},
   "source": [
    "## 三. 子模块和`nn.Sequential`\n",
    "\n",
    "### 3.1 如果不使用`nn.Sequential`的话，也可以这样写\n",
    "```py\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "这里每个 `self.fc1`, `self.fc2`, `self.fc3` 都是子模块，`SimpleNet` 是一个“大模块”，它管理所有子模块的参数。\n",
    "\n",
    "### 3.2 利用`nn.Sequential`(类似于搭积木)\n",
    "\n",
    "`nn.Sequential` 本质就是一个“按顺序执行的一串模块”：\n",
    "\n",
    "```py\n",
    "self.linear_relu_stack = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10)\n",
    ")\n",
    "```\n",
    "\n",
    "它的`forward`相当于\n",
    "\n",
    "```py\n",
    "def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6725c8d",
   "metadata": {},
   "source": [
    "## 四、`nn.Module` 和训练循环的关系\n",
    "\n",
    "### 4.1 `parameters()`：交给优化器的就是它\n",
    "\n",
    "当我们写：\n",
    "\n",
    "```python\n",
    "model = MLP().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "```\n",
    "\n",
    "这里的 `model.parameters()` 就是 `nn.Module` 提供的一个方法：\n",
    "\n",
    "* 会自动遍历所有**子模块**，收集里面的 `nn.Parameter`（可学习的权重）\n",
    "* 返回一个可迭代对象，优化器就能更新这些参数\n",
    "\n",
    "所以你只要**正确地在 `__init__` 里挂层到 `self.xxx` 上**，剩下交给 `model.parameters()` 就行。\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 `to()` / `cuda()`：模型整体搬设备\n",
    "\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MLP().to(device)\n",
    "```\n",
    "\n",
    "* `model.to(device)` 会把所有参数和缓冲（如 `running_mean` 等）都搬到对应设备\n",
    "* 之后我们只要保证输入 `x` 也 `.to(device)`，就能在 GPU 上跑前向 + 反向\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 `train()` / `eval()`：切换训练 / 测试模式\n",
    "\n",
    "```python\n",
    "model.train()   # 训练模式\n",
    "# ...\n",
    "model.eval()    # 测试/推理模式\n",
    "```\n",
    "\n",
    "这两个方法是 `nn.Module` 提供的，它们会：\n",
    "\n",
    "* 设置 `self.training = True/False`\n",
    "* 递归调用所有子模块的 `train()` / `eval()`\n",
    "\n",
    "为什么需要这个？\n",
    "\n",
    "* 像 `nn.Dropout`、`nn.BatchNorm` 这种层在 train/eval 下行为不同：\n",
    "\n",
    "  * Dropout：训练时随机置零部分神经元，测试时关闭随机（使用缩放后的期望）\n",
    "  * BatchNorm：训练时用 batch 的统计量，测试时用滑动平均统计量\n",
    "\n",
    "如果我们不写 `model.train()` / `model.eval()`：\n",
    "\n",
    "* 在测试时 Dropout 仍然随机，会导致预测不稳定\n",
    "* BatchNorm 不会正确使用训练好的统计量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
